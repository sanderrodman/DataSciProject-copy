{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aa64a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b92488",
   "metadata": {},
   "source": [
    "# __Data Collection and Cleaning:__\n",
    "\n",
    "The data that we collected was from multiple different dataframes (more information below in the data description). We made sure to find data that matched in terms of scope (timeframe, location, etc.), but the data was structured in different ways. So, we clean up and merge all of the data below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a639fb",
   "metadata": {},
   "source": [
    "First, we will read in all of the data that we collected regarding the sale of houses from Jan 2018-Sep 2023. This data was taken from https://www.zillow.com/research/data/, and we downloaded 4 data tables from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4136dbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_listings = pd.read_csv(\"new-listings.csv\")\n",
    "df_median_days_to_pending = pd.read_csv(\"median_days_to_pending_month.csv\")\n",
    "df_mean_sale_to_list = pd.read_csv(\"mean_sale_to_list_month.csv\")\n",
    "df_pct_sold_below_list = pd.read_csv(\"pct_sold_below_list_month.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97a26e9",
   "metadata": {},
   "source": [
    "First, we will work with the `df_new_listings` dataframe. The data consists of the number of listings that were posted each month for various regions around the United States. Our project is only concerned with the general US–this is the data found in the first row of the data set. Since we’re examining information on the whole nation and not specific areas, the columns that describe the locations are no longer necessary. We will remove them by using SQL. We print a list of the column names before, and the dataframe after to ensure that the process was done correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6819de4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original column names:\n",
      "Index(['RegionID', 'SizeRank', 'RegionName', 'RegionType', 'StateName',\n",
      "       '2018-03-31', '2018-04-30', '2018-05-31', '2018-06-30', '2018-07-31',\n",
      "       '2018-08-31', '2018-09-30', '2018-10-31', '2018-11-30', '2018-12-31',\n",
      "       '2019-01-31', '2019-02-28', '2019-03-31', '2019-04-30', '2019-05-31',\n",
      "       '2019-06-30', '2019-07-31', '2019-08-31', '2019-09-30', '2019-10-31',\n",
      "       '2019-11-30', '2019-12-31', '2020-01-31', '2020-02-29', '2020-03-31',\n",
      "       '2020-04-30', '2020-05-31', '2020-06-30', '2020-07-31', '2020-08-31',\n",
      "       '2020-09-30', '2020-10-31', '2020-11-30', '2020-12-31', '2021-01-31',\n",
      "       '2021-02-28', '2021-03-31', '2021-04-30', '2021-05-31', '2021-06-30',\n",
      "       '2021-07-31', '2021-08-31', '2021-09-30', '2021-10-31', '2021-11-30',\n",
      "       '2021-12-31', '2022-01-31', '2022-02-28', '2022-03-31', '2022-04-30',\n",
      "       '2022-05-31', '2022-06-30', '2022-07-31', '2022-08-31', '2022-09-30',\n",
      "       '2022-10-31', '2022-11-30', '2022-12-31', '2023-01-31', '2023-02-28',\n",
      "       '2023-03-31', '2023-04-30', '2023-05-31', '2023-06-30', '2023-07-31',\n",
      "       '2023-08-31', '2023-09-30'],\n",
      "      dtype='object')\n",
      "\n",
      "Dataframe after removing unneeded columns:\n",
      "   2018-03-31  2018-04-30  2018-05-31  2018-06-30  2018-07-31  2018-08-31  \\\n",
      "0   1421529.0   1500194.0   1592414.0   1660614.0   1709142.0   1733388.0   \n",
      "1     73707.0     80345.0     85864.0     90067.0     91881.0     91252.0   \n",
      "2     21998.0     23784.0     25605.0     27109.0     28811.0     29874.0   \n",
      "3     38581.0     42253.0     45757.0     47492.0     48984.0     49782.0   \n",
      "4     24043.0     25876.0     28225.0     30490.0     32408.0     33567.0   \n",
      "\n",
      "   2018-09-30  2018-10-31  2018-11-30  2018-12-31  ...  2022-12-31  \\\n",
      "0   1723098.0   1701372.0   1642145.0   1541994.0  ...    955889.0   \n",
      "1     90050.0     89340.0     87186.0     81944.0  ...     50400.0   \n",
      "2     30428.0     30606.0     30035.0     28252.0  ...     18604.0   \n",
      "3     49630.0     48916.0     46495.0     42304.0  ...     25674.0   \n",
      "4     33512.0     32589.0     31265.0     29283.0  ...     19985.0   \n",
      "\n",
      "   2023-01-31  2023-02-28  2023-03-31  2023-04-30  2023-05-31  2023-06-30  \\\n",
      "0    884129.0    830977.0    835429.0    845834.0    880510.0    907228.0   \n",
      "1     45916.0     42796.0     43253.0     44197.0     45829.0     46198.0   \n",
      "2     16767.0     15382.0     15214.0     15103.0     15283.0     15378.0   \n",
      "3     22770.0     20829.0     20922.0     21214.0     22091.0     22706.0   \n",
      "4     18560.0     17106.0     17053.0     17959.0     19987.0     21744.0   \n",
      "\n",
      "   2023-07-31  2023-08-31  2023-09-30  \n",
      "0    930911.0    950306.0    959171.0  \n",
      "1     45506.0     43955.0     42485.0  \n",
      "2     15689.0     15968.0     16017.0  \n",
      "3     23228.0     23568.0     23560.0  \n",
      "4     22748.0     23251.0     23317.0  \n",
      "\n",
      "[5 rows x 67 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"Original column names:\")\n",
    "print(df_new_listings.columns)\n",
    "df_new_listings = duckdb.sql(\"SELECT * EXCLUDE ('RegionID', \\\n",
    "    'SizeRank', 'RegionName', 'RegionType', 'StateName') \\\n",
    "    FROM df_new_listings\").df()\n",
    "print(\"\\nDataframe after removing unneeded columns:\")\n",
    "print(df_new_listings.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d8561e",
   "metadata": {},
   "source": [
    "Now, we want to reshape our data (which is currently only 1 single row) to be vertical, as this will be more useful once we try to merge the rest of the data together. Since our data is currently a series (just the first row of the original table), we will do this by making a new dataframe using the series. Also for the sake of merging, we’ll create `Year` and `Month` columns by turning `Date` to a datetime and then extracting the month and year. We’ll print the first five rows of the new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9920b855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Date  New_Listings  Year  Month\n",
      "2018-03-31 2018-03-31     1421529.0  2018      3\n",
      "2018-04-30 2018-04-30     1500194.0  2018      4\n",
      "2018-05-31 2018-05-31     1592414.0  2018      5\n",
      "2018-06-30 2018-06-30     1660614.0  2018      6\n",
      "2018-07-31 2018-07-31     1709142.0  2018      7\n"
     ]
    }
   ],
   "source": [
    "df_new_listings = df_new_listings.iloc[0]\n",
    "df_new_listings = pd.DataFrame({'Date': df_new_listings.index, \\\n",
    "                                      'New_Listings': df_new_listings})\n",
    "df_new_listings['Date'] = pd.to_datetime(df_new_listings['Date'])\n",
    "df_new_listings['Year'] = df_new_listings['Date'].dt.year\n",
    "df_new_listings['Month'] = df_new_listings['Date'].dt.month\n",
    "\n",
    "\n",
    "print(df_new_listings.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d2888e",
   "metadata": {},
   "source": [
    "We will now do the same thing for `df_median_days_to_pending`. All of the data that we got from Zillow came in the same format, so following the same steps as above will allow us to make our reshaped data consistent. So, below we take the first row, representing data for the whole US, then remove unneeded columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97e001cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_median_days_to_pending = duckdb.sql(\"SELECT * EXCLUDE ('RegionID', \\\n",
    "    'SizeRank', 'RegionName', 'RegionType', 'StateName') \\\n",
    "    FROM df_median_days_to_pending\").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f3ab49",
   "metadata": {},
   "source": [
    "Now we turn the data into a new data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f9cf60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_median_days = df_median_days_to_pending.iloc[0]\n",
    "df_median_days_to_pending = pd.DataFrame({'Date': \\\n",
    "                                       average_median_days.index, \\\n",
    "                                       'Days_To_Pending': \\\n",
    "                                          average_median_days})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe257d4",
   "metadata": {},
   "source": [
    "Now, add columns that we will use to merge, and print the first five rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b97035d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Date  Days_To_Pending  Year  Month\n",
      "2018-01-31 2018-01-31             47.0  2018      1\n",
      "2018-02-28 2018-02-28             25.0  2018      2\n",
      "2018-03-31 2018-03-31             20.0  2018      3\n",
      "2018-04-30 2018-04-30             18.0  2018      4\n",
      "2018-05-31 2018-05-31             18.0  2018      5\n"
     ]
    }
   ],
   "source": [
    "df_median_days_to_pending['Date'] = \\\n",
    "pd.to_datetime(df_median_days_to_pending['Date'])\n",
    "\n",
    "df_median_days_to_pending['Year'] = \\\n",
    "df_median_days_to_pending['Date'].dt.year\n",
    "\n",
    "df_median_days_to_pending['Month'] = \\\n",
    "df_median_days_to_pending['Date'].dt.month\n",
    "\n",
    "print(df_median_days_to_pending.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eee78f",
   "metadata": {},
   "source": [
    "`df_pct_sold_below_list` will follow the same procedure, and we will print the first five rows.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19eb07fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pct_sold_below_list = duckdb.sql(\"SELECT * EXCLUDE ('RegionID', \\\n",
    "    'SizeRank', 'RegionName', 'RegionType', 'StateName') \\\n",
    "    FROM df_pct_sold_below_list\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6092c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Date  Percent_Sold_Below_List  Year  Month\n",
      "2018-01-31 2018-01-31                 0.642516  2018      1\n",
      "2018-02-28 2018-02-28                 0.619052  2018      2\n",
      "2018-03-31 2018-03-31                 0.577498  2018      3\n",
      "2018-04-30 2018-04-30                 0.551780  2018      4\n",
      "2018-05-31 2018-05-31                 0.533494  2018      5\n"
     ]
    }
   ],
   "source": [
    "average_sold_below = df_pct_sold_below_list.iloc[0]\n",
    "df_pct_sold_below_list = pd.DataFrame({'Date': \\\n",
    "                                       average_sold_below.index,\\\n",
    "                            'Percent_Sold_Below_List': \\\n",
    "                                       average_sold_below})\n",
    "df_pct_sold_below_list['Date'] = \\\n",
    "    pd.to_datetime(df_pct_sold_below_list['Date'])\n",
    "df_pct_sold_below_list['Year'] = df_pct_sold_below_list['Date'].dt.year\n",
    "df_pct_sold_below_list['Month'] = df_pct_sold_below_list['Date'].dt.month\n",
    "print(df_pct_sold_below_list.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed17f151",
   "metadata": {},
   "source": [
    "Now, again, `df_mean_sale_to_list` will be cleaned using the same procedure. The first five rows are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa352834",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_sale_to_list = duckdb.sql(\"SELECT * EXCLUDE ('RegionID', \\\n",
    "    'SizeRank', 'RegionName', 'RegionType', \\\n",
    "    'StateName') FROM df_mean_sale_to_list\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66cd05f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Date  Sale_To_List_Price_Ratio  Year  Month\n",
      "2018-01-31 2018-01-31                  0.976664  2018      1\n",
      "2018-02-28 2018-02-28                  0.978713  2018      2\n",
      "2018-03-31 2018-03-31                  0.983239  2018      3\n",
      "2018-04-30 2018-04-30                  0.986186  2018      4\n",
      "2018-05-31 2018-05-31                  0.987151  2018      5\n"
     ]
    }
   ],
   "source": [
    "df_mean_sale_to_list = df_mean_sale_to_list.iloc[0]\n",
    "df_mean_sale_to_list = pd.DataFrame({'Date': \\\n",
    "                                       df_mean_sale_to_list.index, \\\n",
    "                                       'Sale_To_List_Price_Ratio': \\\n",
    "                                       df_mean_sale_to_list})\n",
    "df_mean_sale_to_list['Date'] = \\\n",
    "pd.to_datetime(df_mean_sale_to_list['Date'])\n",
    "df_mean_sale_to_list['Year'] = df_mean_sale_to_list['Date'].dt.year\n",
    "df_mean_sale_to_list['Month'] = df_mean_sale_to_list['Date'].dt.month\n",
    "print(df_mean_sale_to_list.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d80b00",
   "metadata": {},
   "source": [
    "Next, we read in the data that we found on birth and mortality rates, which was taken from https://wonder.cdc.gov/natality-expanded-current.html and https://wonder.cdc.gov/ucd-icd10-expanded.html, respectively. As this data was retrieved by submitting a request, we were unable to scrape it, so we took the data, and turned it into a Excel sheet. Some manual cleaning then had to occur: there were rows after each year that totaled up the numbers for the year, so those were deleted, and the \"Year\" column was expanded to fill in all of the rows (originally only filled out for January of each year). Then the data was converted to a csv. In the dataset, numbers in the thousands contain commas, so we specify that to turn them into ints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a81ef8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year     Month  Births\n",
      "0  2018   January  314808\n",
      "1  2018  February  284250\n",
      "2  2018     March  316044\n",
      "3  2018     April  298394\n",
      "4  2018       May  320622\n"
     ]
    }
   ],
   "source": [
    "birth_df = pd.read_csv(\"birth_rate.csv\", thousands = \",\")\n",
    "print(birth_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef3e9af",
   "metadata": {},
   "source": [
    "When looking at these later datasets that we found, we noticed that they all had their months formatted differently. This would make merging difficult, so for these we made sure that the months were all in \"mmm\" format. We do it for `birth_df` here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c57f30f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year Month  Births\n",
      "0  2018   jan  314808\n",
      "1  2018   feb  284250\n",
      "2  2018   mar  316044\n",
      "3  2018   apr  298394\n",
      "4  2018   may  320622\n"
     ]
    }
   ],
   "source": [
    "birth_df[\"Month\"] = birth_df[\"Month\"].apply(lambda x: x[:3].lower())\n",
    "print(birth_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403e990c",
   "metadata": {},
   "source": [
    "Now we will read in the mortality rate dataset, and do the same thing that we did with `birth_df`: address the commas, and put the months into the format that we wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7695d30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year Month  Deaths\n",
      "0  2018   jan  286744\n",
      "1  2018   feb  236998\n",
      "2  2018   mar  248805\n",
      "3  2018   apr  233164\n",
      "4  2018   may  228772\n"
     ]
    }
   ],
   "source": [
    "mortality_df = pd.read_csv(\"mortality_rate.csv\", thousands = \",\")\n",
    "mortality_df[\"Month\"] = \\\n",
    "    mortality_df[\"Month\"].apply(lambda x: x[:3].lower())\n",
    "print(mortality_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4064a502",
   "metadata": {},
   "source": [
    "Now, we will work with the data on inflation and unemployment rates. The inflation data came from https://www.usinflationcalculator.com/inflation/current-inflation-rates/ and the unemployment data came from https://data.bls.gov/timeseries/LNS14000000. We got the data as xlsx files, and the converted to csv. Both contained data that was before 2018, so we took only the rows that were for 2018 or later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda04066",
   "metadata": {},
   "source": [
    "These datasets are going to require more cleaning than `birth_df` and `mortality_df`, so we will print out what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e0035ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year  Jan  Feb  Mar  Apr  May  Jun  Jul  Aug  Sep  Oct  Nov  Dec\n",
      "0  2023  6.4  6.0  5.0  4.9  4.0  3.0  3.2  3.7  3.7  NaN  NaN  NaN\n",
      "1  2022  7.5  7.9  8.5  8.3  8.6  9.1  8.5  8.3  8.2  7.7  7.1  6.5\n",
      "2  2021  1.4  1.7  2.6  4.2  5.0  5.4  5.4  5.3  5.4  6.2  6.8  7.0\n",
      "3  2020  2.5  2.3  1.5  0.3  0.1  0.6  1.0  1.3  1.4  1.2  1.2  1.4\n",
      "4  2019  1.6  1.5  1.9  2.0  1.8  1.6  1.8  1.7  1.7  1.8  2.1  2.3\n",
      "5  2018  2.1  2.2  2.4  2.5  2.8  2.9  2.9  2.7  2.3  2.5  2.2  1.9\n",
      "    Year  Jan  Feb  Mar   Apr   May   Jun   Jul  Aug  Sep  Oct  Nov  Dec\n",
      "5   2018  4.0  4.1  4.0   4.0   3.8   4.0   3.8  3.8  3.7  3.8  3.8  3.9\n",
      "6   2019  4.0  3.8  3.8   3.6   3.7   3.6   3.7  3.7  3.5  3.6  3.6  3.6\n",
      "7   2020  3.5  3.5  4.4  14.7  13.2  11.0  10.2  8.4  7.9  6.9  6.7  6.7\n",
      "8   2021  6.3  6.2  6.1   6.1   5.8   5.9   5.4  5.2  4.8  4.5  4.2  3.9\n",
      "9   2022  4.0  3.8  3.6   3.6   3.6   3.6   3.5  3.7  3.5  3.7  3.6  3.5\n",
      "10  2023  3.4  3.6  3.5   3.4   3.7   3.6   3.5  3.8  3.8  NaN  NaN  NaN\n"
     ]
    }
   ],
   "source": [
    "inflation_df = pd.read_csv(\"inflation_rate.csv\")\n",
    "\n",
    "unemployment_df = pd.read_csv(\"unemployment_rate.csv\")\n",
    "unemployment_df = unemployment_df.loc[unemployment_df['Year'] >= 2018]\n",
    "\n",
    "print(inflation_df)\n",
    "print(unemployment_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd89e9b",
   "metadata": {},
   "source": [
    "Currently, they are wide dataframes. To be consistent with all of the other data, we need to have the month and years as columns, so both of these dataframes need reshaping. We will also standardize the representation of the months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42381ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year Month  Inflation_Rate\n",
      "0  2023   jan             6.4\n",
      "1  2022   jan             7.5\n",
      "2  2021   jan             1.4\n",
      "3  2020   jan             2.5\n",
      "4  2019   jan             1.6\n"
     ]
    }
   ],
   "source": [
    "inflation_df = inflation_df.melt(id_vars=\"Year\", \\\n",
    "                                 var_name=\"Month\", \\\n",
    "                                 value_name =\"Inflation_Rate\")\n",
    "inflation_df[\"Month\"] = \\\n",
    "    inflation_df[\"Month\"].apply(lambda x: x[:3].lower())\n",
    "print(inflation_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db78d3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year Month  Unemployment_Rate\n",
      "0  2018   jan                4.0\n",
      "1  2019   jan                4.0\n",
      "2  2020   jan                3.5\n",
      "3  2021   jan                6.3\n",
      "4  2022   jan                4.0\n"
     ]
    }
   ],
   "source": [
    "unemployment_df = unemployment_df.melt(id_vars=\"Year\", \\\n",
    "                                       var_name=\"Month\", \\\n",
    "                                       value_name =\"Unemployment_Rate\")\n",
    "unemployment_df[\"Month\"] = \\\n",
    "                unemployment_df[\"Month\"].apply(lambda x: x[:3].lower())\n",
    "print(unemployment_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e649b8ba",
   "metadata": {},
   "source": [
    "__Merging Dataframes__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14712dac",
   "metadata": {},
   "source": [
    "Now that we have gotten our data to be in the same structure, we can merge them together into one larger data set. We will start by merging all of our data on the sale of houses into one dataframe that we will call `housing_data_df`. We print the head to get a preview of what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92b06611",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data_df = duckdb.sql(\"SELECT a.Year, a.Month, \\\n",
    "New_Listings, Days_To_Pending, \\\n",
    "Percent_Sold_Below_List, Sale_To_List_Price_Ratio \\\n",
    "FROM df_median_days_to_pending AS a \\\n",
    "LEFT JOIN df_new_listings AS b \\\n",
    "    ON a.Year = b.Year AND a.Month = b.Month \\\n",
    "LEFT JOIN df_pct_sold_below_list AS c \\\n",
    "    ON a.Year = c.Year AND a.Month = c.Month \\\n",
    "LEFT JOIN df_mean_sale_to_list AS d \\\n",
    "    ON a.Year = d.Year AND a.Month = d.Month \\\n",
    "ORDER BY a.Year, a.Month\").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef0edcd",
   "metadata": {},
   "source": [
    "Now, the data here is all merged, and in order. However, we had these dataframes represent months using integers, so we need to change this column so it matches our other data.\n",
    "\n",
    "https://stackoverflow.com/questions/14533709/basic-python-programming-to-convert-month-number-to-month-name-using-dictionary\n",
    "\n",
    "Using the function provided in stackoverflow, we converted the column of month integers to strings to match the data type in the other dataframes. We used the pandas apply function with a lambda function to apply the dictionary to each value in the column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf406c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthDict={1:'jan', 2:'feb', 3:'mar', 4:'apr', 5:'may', \\\n",
    "           6:'jun', 7:'jul', 8:'aug', 9:'sep', 10:'oct', \\\n",
    "           11:'nov', 12:'dec'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7128649a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year Month  New_Listings  Days_To_Pending  Percent_Sold_Below_List  \\\n",
      "0  2018   jan           NaN             47.0                 0.642516   \n",
      "1  2018   feb           NaN             25.0                 0.619052   \n",
      "2  2018   mar     1421529.0             20.0                 0.577498   \n",
      "3  2018   apr     1500194.0             18.0                 0.551780   \n",
      "4  2018   may     1592414.0             18.0                 0.533494   \n",
      "\n",
      "   Sale_To_List_Price_Ratio  \n",
      "0                  0.976664  \n",
      "1                  0.978713  \n",
      "2                  0.983239  \n",
      "3                  0.986186  \n",
      "4                  0.987151  \n"
     ]
    }
   ],
   "source": [
    "housing_data_df[\"Month\"] = \\\n",
    "    housing_data_df[\"Month\"].apply(lambda x: monthDict[x])\n",
    "\n",
    "print (housing_data_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4602876",
   "metadata": {},
   "source": [
    "Now, we can move on to merging in the other four data tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f1de138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>New_Listings</th>\n",
       "      <th>Days_To_Pending</th>\n",
       "      <th>Percent_Sold_Below_List</th>\n",
       "      <th>Sale_To_List_Price_Ratio</th>\n",
       "      <th>Births</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Inflation_Rate</th>\n",
       "      <th>Unemployment_Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018</td>\n",
       "      <td>jan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.642516</td>\n",
       "      <td>0.976664</td>\n",
       "      <td>314808.0</td>\n",
       "      <td>286744.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018</td>\n",
       "      <td>feb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.619052</td>\n",
       "      <td>0.978713</td>\n",
       "      <td>284250.0</td>\n",
       "      <td>236998.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018</td>\n",
       "      <td>mar</td>\n",
       "      <td>1421529.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.577498</td>\n",
       "      <td>0.983239</td>\n",
       "      <td>316044.0</td>\n",
       "      <td>248805.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018</td>\n",
       "      <td>apr</td>\n",
       "      <td>1500194.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.551780</td>\n",
       "      <td>0.986186</td>\n",
       "      <td>298394.0</td>\n",
       "      <td>233164.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018</td>\n",
       "      <td>may</td>\n",
       "      <td>1592414.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.533494</td>\n",
       "      <td>0.987151</td>\n",
       "      <td>320622.0</td>\n",
       "      <td>228772.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2023</td>\n",
       "      <td>may</td>\n",
       "      <td>880510.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.419565</td>\n",
       "      <td>0.997535</td>\n",
       "      <td>NaN</td>\n",
       "      <td>252046.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2023</td>\n",
       "      <td>jun</td>\n",
       "      <td>907228.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.399708</td>\n",
       "      <td>1.000553</td>\n",
       "      <td>NaN</td>\n",
       "      <td>240140.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2023</td>\n",
       "      <td>jul</td>\n",
       "      <td>930911.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.411186</td>\n",
       "      <td>0.998900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>244278.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2023</td>\n",
       "      <td>aug</td>\n",
       "      <td>950306.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.434983</td>\n",
       "      <td>0.995926</td>\n",
       "      <td>NaN</td>\n",
       "      <td>238615.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2023</td>\n",
       "      <td>sep</td>\n",
       "      <td>959171.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.991654</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Year Month  New_Listings  Days_To_Pending  Percent_Sold_Below_List  \\\n",
       "0   2018   jan           NaN             47.0                 0.642516   \n",
       "1   2018   feb           NaN             25.0                 0.619052   \n",
       "2   2018   mar     1421529.0             20.0                 0.577498   \n",
       "3   2018   apr     1500194.0             18.0                 0.551780   \n",
       "4   2018   may     1592414.0             18.0                 0.533494   \n",
       "..   ...   ...           ...              ...                      ...   \n",
       "64  2023   may      880510.0             10.0                 0.419565   \n",
       "65  2023   jun      907228.0             11.0                 0.399708   \n",
       "66  2023   jul      930911.0             12.0                 0.411186   \n",
       "67  2023   aug      950306.0             13.0                 0.434983   \n",
       "68  2023   sep      959171.0             15.0                      NaN   \n",
       "\n",
       "    Sale_To_List_Price_Ratio    Births    Deaths  Inflation_Rate  \\\n",
       "0                   0.976664  314808.0  286744.0             2.1   \n",
       "1                   0.978713  284250.0  236998.0             2.2   \n",
       "2                   0.983239  316044.0  248805.0             2.4   \n",
       "3                   0.986186  298394.0  233164.0             2.5   \n",
       "4                   0.987151  320622.0  228772.0             2.8   \n",
       "..                       ...       ...       ...             ...   \n",
       "64                  0.997535       NaN  252046.0             4.0   \n",
       "65                  1.000553       NaN  240140.0             3.0   \n",
       "66                  0.998900       NaN  244278.0             3.2   \n",
       "67                  0.995926       NaN  238615.0             3.7   \n",
       "68                  0.991654       NaN       NaN             3.7   \n",
       "\n",
       "    Unemployment_Rate  \n",
       "0                 4.0  \n",
       "1                 4.1  \n",
       "2                 4.0  \n",
       "3                 4.0  \n",
       "4                 3.8  \n",
       "..                ...  \n",
       "64                3.7  \n",
       "65                3.6  \n",
       "66                3.5  \n",
       "67                3.8  \n",
       "68                3.8  \n",
       "\n",
       "[69 rows x 10 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_data_df = duckdb.sql(\"SELECT a.*, Births, Deaths, Inflation_rate, \\\n",
    "Unemployment_Rate FROM housing_data_df AS a \\\n",
    "LEFT JOIN mortality_df AS b ON a.Year = b.Year AND a.Month = b.Month \\\n",
    "LEFT JOIN inflation_df AS c ON a.Year = c.Year AND a.Month = c.Month \\\n",
    "LEFT JOIN unemployment_df AS d ON a.Year = d.Year AND a.Month = d.Month \\\n",
    "LEFT JOIN birth_df AS e on a.Year = e.Year AND a.Month = e.Month \\\n",
    "ORDER BY a.YEAR\").df()\n",
    "housing_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1de350-2fb0-426b-8459-5659bd9748fc",
   "metadata": {},
   "source": [
    "We'll make a few final touches to our dataset. First of all, it was mentioned to us that these events probably do not have an immediate impact on the economy, and therefore will not have an immediate impact on the economy, and therefore won't have an immediate impact on the sale of houses. We will adjust the `Deaths` column by 3 months to account for time between when an estate is resold [1]. It would be interesting to do more analyses with different adjustments of time to determine how long it actually takes for impacts to be seen.\n",
    "\n",
    "We will also create a column that measures the amount of months since the starting date of our data, January 2018. This will be useful in our later analyses where we need to account for time.\n",
    "\n",
    "[1] https://www.homelight.com/blog/how-long-does-executor-have-to-sell-a-house/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd348eeb-bd51-4452-b59d-1384b223db4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>New_Listings</th>\n",
       "      <th>Days_To_Pending</th>\n",
       "      <th>Percent_Sold_Below_List</th>\n",
       "      <th>Sale_To_List_Price_Ratio</th>\n",
       "      <th>Births</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Inflation_Rate</th>\n",
       "      <th>Unemployment_Rate</th>\n",
       "      <th>Months_Since_Jan_2018</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018</td>\n",
       "      <td>jan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.642516</td>\n",
       "      <td>0.976664</td>\n",
       "      <td>314808.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018</td>\n",
       "      <td>feb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.619052</td>\n",
       "      <td>0.978713</td>\n",
       "      <td>284250.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018</td>\n",
       "      <td>mar</td>\n",
       "      <td>1421529.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.577498</td>\n",
       "      <td>0.983239</td>\n",
       "      <td>316044.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018</td>\n",
       "      <td>apr</td>\n",
       "      <td>1500194.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.551780</td>\n",
       "      <td>0.986186</td>\n",
       "      <td>298394.0</td>\n",
       "      <td>286744.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018</td>\n",
       "      <td>may</td>\n",
       "      <td>1592414.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.533494</td>\n",
       "      <td>0.987151</td>\n",
       "      <td>320622.0</td>\n",
       "      <td>236998.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>3.8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2023</td>\n",
       "      <td>may</td>\n",
       "      <td>880510.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.419565</td>\n",
       "      <td>0.997535</td>\n",
       "      <td>NaN</td>\n",
       "      <td>249084.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2023</td>\n",
       "      <td>jun</td>\n",
       "      <td>907228.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.399708</td>\n",
       "      <td>1.000553</td>\n",
       "      <td>NaN</td>\n",
       "      <td>269173.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2023</td>\n",
       "      <td>jul</td>\n",
       "      <td>930911.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.411186</td>\n",
       "      <td>0.998900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>251959.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2023</td>\n",
       "      <td>aug</td>\n",
       "      <td>950306.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.434983</td>\n",
       "      <td>0.995926</td>\n",
       "      <td>NaN</td>\n",
       "      <td>252046.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2023</td>\n",
       "      <td>sep</td>\n",
       "      <td>959171.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.991654</td>\n",
       "      <td>NaN</td>\n",
       "      <td>240140.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Year Month  New_Listings  Days_To_Pending  Percent_Sold_Below_List  \\\n",
       "0   2018   jan           NaN             47.0                 0.642516   \n",
       "1   2018   feb           NaN             25.0                 0.619052   \n",
       "2   2018   mar     1421529.0             20.0                 0.577498   \n",
       "3   2018   apr     1500194.0             18.0                 0.551780   \n",
       "4   2018   may     1592414.0             18.0                 0.533494   \n",
       "..   ...   ...           ...              ...                      ...   \n",
       "64  2023   may      880510.0             10.0                 0.419565   \n",
       "65  2023   jun      907228.0             11.0                 0.399708   \n",
       "66  2023   jul      930911.0             12.0                 0.411186   \n",
       "67  2023   aug      950306.0             13.0                 0.434983   \n",
       "68  2023   sep      959171.0             15.0                      NaN   \n",
       "\n",
       "    Sale_To_List_Price_Ratio    Births    Deaths  Inflation_Rate  \\\n",
       "0                   0.976664  314808.0       NaN             2.1   \n",
       "1                   0.978713  284250.0       NaN             2.2   \n",
       "2                   0.983239  316044.0       NaN             2.4   \n",
       "3                   0.986186  298394.0  286744.0             2.5   \n",
       "4                   0.987151  320622.0  236998.0             2.8   \n",
       "..                       ...       ...       ...             ...   \n",
       "64                  0.997535       NaN  249084.0             4.0   \n",
       "65                  1.000553       NaN  269173.0             3.0   \n",
       "66                  0.998900       NaN  251959.0             3.2   \n",
       "67                  0.995926       NaN  252046.0             3.7   \n",
       "68                  0.991654       NaN  240140.0             3.7   \n",
       "\n",
       "    Unemployment_Rate  Months_Since_Jan_2018  \n",
       "0                 4.0                      0  \n",
       "1                 4.1                      1  \n",
       "2                 4.0                      2  \n",
       "3                 4.0                      3  \n",
       "4                 3.8                      4  \n",
       "..                ...                    ...  \n",
       "64                3.7                     64  \n",
       "65                3.6                     65  \n",
       "66                3.5                     66  \n",
       "67                3.8                     67  \n",
       "68                3.8                     68  \n",
       "\n",
       "[69 rows x 11 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_data_df['Deaths'] = pd.concat([pd.Series([np.NaN]*3), housing_data_df['Deaths'][:-3]], ignore_index=True)\n",
    "housing_data_df['Months_Since_Jan_2018'] = housing_data_df.index\n",
    "housing_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906242ca",
   "metadata": {},
   "source": [
    "Now, the data is all in one table and looks the way we would like it to. There are still empty cells with missing data, so we will address those when we analyze the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0269c5",
   "metadata": {},
   "source": [
    "Now we will convert the dataframe into a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fdc2b4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data_df.to_csv('housing_data_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
